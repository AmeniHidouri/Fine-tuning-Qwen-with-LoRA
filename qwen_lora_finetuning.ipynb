{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Préparation de l'Environnement et Imports\n",
        "\n",
        "\n",
        "Cette cellule installe toutes les librairies nécessaires et charge les packages Python."
      ],
      "metadata": {
        "id": "ysT-K4vJpEqq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsOUbezRng5q",
        "outputId": "9978db7c-b3bf-4f05-ecc7-1cc5517e4194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 10 21:49:53 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   76C    P0             35W /   70W |    3902MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Vérification de la GPU et installation des librairies\n",
        "!nvidia-smi\n",
        "\n",
        "!pip install -qqq bitsandbytes torch transformers peft accelerate datasets loralib einops trl\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pprint import pprint\n",
        "\n",
        "import bitsandbytes as bnb\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "from trl import DPOConfig, DPOTrainer\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftConfig,\n",
        "    PeftModel,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    PeftModelForCausalLM\n",
        ")\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForSeq2Seq,\n",
        ")\n",
        "\n",
        "# Fonction utilitaire pour vérifier les paramètres entraînés\n",
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "# Vider le cache du GPU avant de lancer le fine-tuning\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Chargement et Configuration LoRA pour Qwen\n",
        "\n",
        "Cette cellule configure spécifiquement le modèle Qwen2.5-0.5B avec les paramètres de quantification et LoRA corrects."
      ],
      "metadata": {
        "id": "TVgkzeHipacC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration du Modèle et de la Quantification ---\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
        "\n",
        "# Configuration BitsAndBytes 4-bit NF4 + BF16 + Double Quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "# Chargement du modèle Qwen quantifié\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token # Configuration du pad token\n",
        "\n",
        "\n",
        "# --- Configuration LoRA (PEFT) pour Qwen ---\n",
        "# Hyperparamètres R=32, Alpha=64, Dropout=0.05\n",
        "config_qwen = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.05,\n",
        "    # Modules cibles Qwen/Llama/Mistral\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias='none',\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Application de LoRA\n",
        "model = get_peft_model(model, config_qwen)\n",
        "print(\"Paramètres entraînables après LoRA :\")\n",
        "print_trainable_parameters(model) # Doit afficher environ 5.29% de paramètres entraînables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyzh058spdVd",
        "outputId": "9e15a4e9-b970-41c1-cfd8-b452f502dc48"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paramètres entraînables après LoRA :\n",
            "trainable params: 17596416 || all params: 332715904 || trainable%: 5.288721034507566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Préparation des Données\n"
      ],
      "metadata": {
        "id": "wb70zAeVpww_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Chargement et Tokenization des Données (OrangeSum) ---\n",
        "data = load_dataset(\"giuliadc/orangesum_5k\")[\"train\"]\n",
        "\n",
        "# ATTENTION : Correction de l'OutOfMemoryError (OOM) en réduisant MAX_LENGTH.\n",
        "# 2000 tokens sont trop pour une T4, nous utilisons 512 tokens.\n",
        "MAX_LENGTH = 512\n",
        "\n",
        "def generate_prompt(data_point):\n",
        "    # Template de Prompt <human> <assistant>\n",
        "    return f\"<human>: Résumez l'article suivant:\\n{data_point['text']}\\n<assistant>: {data_point['reference-summary']}\"\n",
        "\n",
        "def generate_and_tokenize_prompt(data_point):\n",
        "    full_prompt = generate_prompt(data_point) + tokenizer.eos_token\n",
        "    # Troncature ajoutée ici avec MAX_LENGTH pour éviter l'OOM\n",
        "    tokenized_full_prompt = tokenizer(full_prompt, truncation=True, max_length=MAX_LENGTH, return_tensors='pt')\n",
        "\n",
        "    # On peut ignorer le filtre de longueur si la troncature est appliquée ci-dessus\n",
        "    if tokenized_full_prompt.input_ids.shape[1] > MAX_LENGTH:\n",
        "        return None # Devrait être rare avec la troncature\n",
        "\n",
        "    labels = tokenized_full_prompt.input_ids.clone() # Création des labels\n",
        "\n",
        "    # Détermination de l'index de début de réponse pour le masquage (-100)\n",
        "    prompt = full_prompt[:full_prompt.find(\"<assistant>\")] + \"<assistant>:\"\n",
        "    tokenized_prompt = tokenizer(prompt, return_tensors='pt')\n",
        "    end_prompt_idx = tokenized_prompt.input_ids.shape[1] # Index de fin du prompt\n",
        "\n",
        "    # Masquage des tokens de l'instruction (Completion-Only)\n",
        "    labels[:, :end_prompt_idx] = -100\n",
        "\n",
        "    return {\n",
        "        'input_ids': tokenized_full_prompt.input_ids.flatten(),\n",
        "        'labels': labels.flatten(),\n",
        "        'attention_mask': tokenized_full_prompt.attention_mask.flatten(),\n",
        "    }\n",
        "\n",
        "# Tokenization et masquage du jeu de données\n",
        "data = data.shuffle(seed=42).map(generate_and_tokenize_prompt, remove_columns=['id', 'text', 'reference-summary'])\n",
        "data = data.filter(lambda x: x is not None) # Retirer les None s'il en reste"
      ],
      "metadata": {
        "id": "k8Pimb1epy2x"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Entraînement du Modèle (Fine-Tuning)\n",
        "\n",
        "Cette cellule configure le Trainer avec les arguments d'entraînement optimisés et lance le fine-tuning."
      ],
      "metadata": {
        "id": "AT4tg4xIqGt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Arguments d'Entraînement Optimisés ---\n",
        "OUTPUT_DIR = \"qwen_finetuning_experiments\"\n",
        "\n",
        "training_args = transformers.TrainingArguments(\n",
        "    per_device_train_batch_size=1, # Taille de lot minimale\n",
        "    gradient_accumulation_steps=16, # Grande accumulation pour simuler lot plus grand\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=5e-4,\n",
        "    bf16=True,\n",
        "    save_total_limit=3,\n",
        "    logging_steps=20,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    max_steps=200,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.01,\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "# --- Configuration du Trainer ---\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data,\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model), # Padding dynamique\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "\n",
        "print(\"Lancement du Fine-Tuning de Qwen 0.5B (MAX_LENGTH=512)...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "2F2QSF1gqNDS",
        "outputId": "811228b1-3857-4b1c-881f-ce97f0c26b25"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lancement du Fine-Tuning de Qwen 0.5B (MAX_LENGTH=512)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 40:12, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1183050.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>4.958800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>8.367600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>13.207500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.202700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>4.433400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>2.061600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>674.202800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>372.690500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.865100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=200, training_loss=118413.42899118424, metrics={'train_runtime': 2426.9043, 'train_samples_per_second': 1.319, 'train_steps_per_second': 0.082, 'total_flos': 3687627372009216.0, 'train_loss': 118413.42899118424, 'epoch': 0.64})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Test et Fusion (Inférence)\n",
        "\n",
        "\n",
        "Une fois l'entraînement terminé (ou interrompu), cette cellule fusionne les adaptateurs LoRA avec le modèle de base pour un déploiement facile et teste la réponse du modèle."
      ],
      "metadata": {
        "id": "vkTUoyIbqRYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Définition des Fonctions de Génération et de Réponse ---\n",
        "\n",
        "def generate_response(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Construit le prompt de chat, génère la réponse du modèle et extrait uniquement\n",
        "    la partie de l'assistant (Completion-Only).\n",
        "    \"\"\"\n",
        "    # 1. Construction du prompt\n",
        "    full_prompt = f\"<human>: {prompt}\\n<assistant>:\"\n",
        "\n",
        "    # Réutilisation des configurations de génération\n",
        "    generation_config = model.generation_config\n",
        "    generation_config.max_new_tokens = 200\n",
        "    generation_config.temperature = 0.7\n",
        "    generation_config.top_p = 0.7\n",
        "    generation_config.num_return_sequences = 1\n",
        "    generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "    generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "    generation_config.do_sample = True\n",
        "\n",
        "    device = \"cuda:0\"\n",
        "    encoding = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # 2. Génération de la réponse\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(\n",
        "            input_ids=encoding.input_ids,\n",
        "            attention_mask=encoding.attention_mask,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # 3. Extraction de la réponse (après '<assistant>:')\n",
        "    assistant_start = \"<assistant>:\"\n",
        "    response_start = response.find(assistant_start)\n",
        "\n",
        "    # Si le format est trouvé, retourne seulement la réponse de l'assistant\n",
        "    if response_start != -1:\n",
        "        return response[response_start + len(assistant_start):].strip()\n",
        "    return response.strip()\n",
        "\n",
        "\n",
        "# --- Test du Modèle après Fine-Tuning ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TEST DU MODÈLE APRÈS FINE-TUNING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Article de test en français (Transparence salariale)\n",
        "article_test = \"\"\"\n",
        "Une petite révolution se prépare. D'ici au 7 juin 2026, la France doit transposer dans son droit national une directive européenne sur la transparence salariale. Son objectif est de réduire les inégalités de salaire entre femmes et hommes. Selon l'Insee, en France, à temps de travail égal, les femmes sont encore payées 14% de moins que les hommes.\n",
        "'À travail égal, rémunération égale. Et pour parvenir à l’égalité de rémunération, il faut de la transparence...', avait déclaré la présidente de la Commission européenne Ursula von der Leyen.\n",
        "\"\"\"\n",
        "\n",
        "prompt_resume = f\"Résumez l'article suivant:\\n{article_test}\"\n",
        "print(f\"-> Résumé de l'article:\\n{generate_response(prompt_resume)}\\n\")\n",
        "\n",
        "# Test sur un prompt hors-distribution (hors-sujet)\n",
        "prompt_ood = \"Do you know the reasons as to why people love coffee so much?\"\n",
        "print(f\"-> Réponse au prompt OOD:\\n{generate_response(prompt_ood)}\")\n",
        "\n",
        "\n",
        "# --- Fusion des Poids LoRA et Sauvegarde Finale ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FUSION ET SAUVEGARDE FINALE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# La fusion des adaptateurs LoRA dans le modèle de base\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Sauvegarde des poids finaux (modèle complet allégé)\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"Modèle fusionné et sauvegardé dans {OUTPUT_DIR}\")\n",
        "print(\"Le modèle est prêt pour le déploiement sans adaptateurs PEFT.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cB32cvdqaTf",
        "outputId": "d0be1502-05f1-4c3b-c627-0023d8688a5f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "TEST DU MODÈLE APRÈS FINE-TUNING\n",
            "==================================================\n",
            "-> Résumé de l'article:\n",
            "Selon les données de l'Insee, la France doit transposer une directive européenne sur la transparence salariale.\n",
            "\n",
            "-> Réponse au prompt OOD:\n",
            "The most popular drink in the world is the coffee. It is a beverage that is enjoyed by people all over the world.\n",
            "\n",
            "==================================================\n",
            "FUSION ET SAUVEGARDE FINALE\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modèle fusionné et sauvegardé dans qwen_finetuning_experiments\n",
            "Le modèle est prêt pour le déploiement sans adaptateurs PEFT.\n"
          ]
        }
      ]
    }
  ]
}